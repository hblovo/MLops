{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b932690c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81449ecf8c37d8eabb5d8797a84499fc",
     "grade": false,
     "grade_id": "cell-f2471a8af23370fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week3 Assignments\n",
    "**Please do the assignments using the `mlops_eng` environment.**\n",
    "\n",
    "This week's assignments will give you some hands-on experience with Optuna (and Ray Tune). Similar to the tutorial of the first week, the [red wine dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) will be used in this week's assignments. \n",
    "\n",
    "**Guidelines for submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. Otherwise your notebook may not pass the tests used in grading.\n",
    "- Some assignments also require you to answer questions (in text) or capture screenshots in order to earn points. Please put your text answers and screenshots in a single PDF file. For each answer and screenshot, please clearly indicate which assignment it corresponds to in your PDF file. Please include the PDF file in your submission.\n",
    "- In Assignments 1 and 2, you'll be asked to save your Optuna Studies in an SQLite database \"optuna.sqlite3\" (the database will be created when you proceed with the assignments). Please also include this database in your submission (**do not change the file name, just keep it as \"optuna.sqlite3\"**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dfe776",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a694eee709e5728fbd55272102e83b23",
     "grade": false,
     "grade_id": "cell-5251f7930ec308de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import platform\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from optuna.samplers import TPESampler\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4b5a4-980b-4bf7-a93c-0841b0ec33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e12dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b218b9c57aee860a174b0096d3817e66",
     "grade": false,
     "grade_id": "cell-907e33c01ce6baa8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure you've installed the right version of lightgbm and xgboost\n",
    "current_platform = platform.system()\n",
    "print(f\"Current platform: {current_platform}\")\n",
    "if current_platform == \"Darwin\" or current_platform == \"Linux\":\n",
    "    if current_platform == \"Darwin\":  # macOS\n",
    "        assert lgb.__version__.startswith(\"4.5.0\"), f\"Wrong version of lightgbm for platform: {current_platform}\"\n",
    "        assert xgb.__version__.startswith(\"2.0.3\"), f\"Wrong version of xgb for platform: {current_platform}\"\n",
    "    elif current_platform == \"Linux\": # Ubuntu\n",
    "        assert lgb.__version__ == \"4.0.0\", f\"Wrong version of lightgbm for platform: {current_platform}\"\n",
    "        assert xgb.__version__ == \"2.0.3\", f\"Wrong version of xgboost for platform: {current_platform}\"\n",
    "    else:\n",
    "        assert False, f\"Unknown platform: {current_platform}\"\n",
    "else:\n",
    "    assert False, f\"Unexpected platform: {current_platform}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476fca2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "359816a57fe72335f6bef7a010d6e38b",
     "grade": false,
     "grade_id": "cell-db5b726e21265212",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is just for the grading purpose\n",
    "def is_being_graded():\n",
    "    \"\"\"\n",
    "    Returns True if the notebook is being executed by the auto-grading tool.\n",
    "    \"\"\"\n",
    "    env = os.environ.get(\"NBGRADER_EXECUTION\")\n",
    "    return env == \"autograde\" or env == \"validate\"\n",
    "\n",
    "\n",
    "# Suppress loggings and warnings when grading the notebook\n",
    "if is_being_graded():\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    mlflow.utils.logging_utils.disable_logging()\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5f155",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad5c08ed5ebc831885975da56fc7304",
     "grade": false,
     "grade_id": "cell-89dcff2b2c701d19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Random seed for making the assignments reproducible\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# MLflow service URI\n",
    "mlflow_tracking_uri = \"http://mlflow-server.local\"\n",
    "\n",
    "# Configure MLflow \n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf832a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "265b527d8f4d3510f639118c82937d2a",
     "grade": false,
     "grade_id": "cell-fdd5de47cceb02fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prepare training and testing data\n",
    "data = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "X = data.drop(\"quality\", axis=1)\n",
    "y = data[\"quality\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f7a58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6005e4fab5d3aa392d73519e73d85383",
     "grade": false,
     "grade_id": "cell-37420489486547e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# An overview of the original dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2bbd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "678b32525cbb26ebe55a665f240f6116",
     "grade": false,
     "grade_id": "cell-02e51dc6b10bf7cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The dimension of train_x is {train_x.shape}\")\n",
    "print(f\"The dimension of train_y is {train_y.shape}\")\n",
    "print(f\"The dimension of test_x is {test_x.shape}\")\n",
    "print(f\"The dimension of test_y is {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1805063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c911f265204417e4c8797d5b75db878e",
     "grade": false,
     "grade_id": "cell-59b310c39429dd2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: The basic use of Optuna (2 points)\n",
    "Your task is to use Optuna to find the optimal hyperparameter combination for the [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) used for predicting red wine quality. **Please use the sklearn API as you did in the first week's assignments.**\n",
    "\n",
    "This assignment has the following requirements:\n",
    "\n",
    "1) Define the objective function (`objective_func`). The target of the optimization is to minimize the MAE (mean absolute error) of the model when evaluating the model against the testing dataset. The hyperparameters to be tuned and their search ranges are shown below. Some of the hyperparameter values are fixed. The hyperparameter values should be sampled in a linear domain if not separately specified. In your objective function, please specify the hyperparameters in the same order as presented in the table.  \n",
    "\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 0.5]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 0.5]                                                              |\n",
    "| min_child_samples | A leaf node should have `min_child_samples` data points to be further splitted. | integer | [20, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value, RANDOM_SEED has been defined as a variable in a previous cell)                                                |\n",
    "\n",
    "2) Define another function named `run_study` that creates and runs a study. Detailed requirements are listed below:\n",
    "    - Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use `RANDOM_SEED` as the seed of the sampler. \n",
    "    - The Optuna study should have a name specified by the `study_name` argument, use the objective function given as the `objective_func` argument and perform `n_trials` trials.\n",
    "    - The study history should be persisted in a relational database specified by the `storage` argument so that the study can be loaded and analyzed later. \n",
    "    - The function should finally return the study.\n",
    "\n",
    "Hints:\n",
    "- [How to sample hyperparameter values in the logarithmic domain?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float)\n",
    "- [How to configure a study to use a specific sampler and persist study history in a specific database?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html#optuna-create-study)\n",
    "- You'll probably see LightGBM throw a bunch of warnings of \"No further splits with positive gain, best gain: -inf\". This warning basically means LightGBM can't find a split that would improve the model's performance at a particular node. You can suppress these warning by setting `verbose=-1` when defining your model, e.g., `lgb.LGBMRegressor(verbose=-1, ...)`\n",
    "\n",
    "**Notes**:\n",
    "- When define the search space for the hyperparameters, please use the names given in the \"Hyperparameter\" column in the table above.\n",
    "- Please **do not** use deprecated methods (e.g, suggest_uniform and suggest_loguniform) when define the search space. \n",
    "\n",
    "*More reading material: If you are interested, [the LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters) explains the use of each hyperparameter in more details.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b96c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02ad6d8ca0e4d5a4b8c4b3fe0d0d39c8",
     "grade": false,
     "grade_id": "cell-15d27f8fdf5190ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_func(trial):\n",
    "    # TODO:\n",
    "    # Define the hyperparameters to be tuned and their search ranges\n",
    "    # Train a model using the sampled hyperparameters\n",
    "    # Evaluate the model using the test dataset\n",
    "    # Return the evaluation metric\n",
    "    ### START CODE HERE\n",
    "    params = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 0.5),\n",
    "        'subsample_freq': 1,\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 0.5),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'verbose': -1 \n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(train_x,train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    mae = mean_absolute_error(test_y,pred)\n",
    "    return mae\n",
    "    ### END CODE HERE\n",
    "\n",
    "def run_study(study_name: str, storage: str, objective_func: callable, n_trials: int) -> optuna.study.Study:\n",
    "    \"\"\"\n",
    "    Create and run an Optuna study.\n",
    "    Args:\n",
    "        study_name: The name of the study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        objective_func: The objective function to be optimized.\n",
    "        n_trials: The number of trials the study should perform.\n",
    "    Returns:\n",
    "        A Study object.\n",
    "    \"\"\"\n",
    "    # Delete the study if it already exists\n",
    "    if study_name in optuna.get_all_study_names(storage=storage):\n",
    "        optuna.delete_study(study_name=study_name, storage=storage)\n",
    "\n",
    "    # TODO: Create (and run) the study and record the history in the storage\n",
    "    ### START CODE HERE\n",
    "    sampler = TPESampler(seed = RANDOM_SEED)\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage,\n",
    "        sampler=sampler,\n",
    "        direction='minimize'\n",
    "    )\n",
    "    study.optimize(objective_func,n_trials=n_trials,show_progress_bar=False)\n",
    "\n",
    "    return study\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323727f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26db5eba7a4f29ecab0a4ef903db0746",
     "grade": false,
     "grade_id": "cell-f4795fd66796fd8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign \"lgbm-wine-1\" as the study_name to the Optuna study in the first assignment.\n",
    "study_name_1 = \"lgbm-wine-1\"\n",
    "\n",
    "# For this assignment, it is enough to use a simple sqlite3 database for persisting study history\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "# When grading the notebook, the study will be loaded from the submitted database\n",
    "study_1 = (\n",
    "    run_study(\n",
    "        study_name=study_name_1,\n",
    "        storage=storage,\n",
    "        objective_func=objective_func,\n",
    "        n_trials=100,\n",
    "    )\n",
    "    if not is_being_graded()\n",
    "    else optuna.load_study(study_name=study_name_1, storage=storage)\n",
    ")\n",
    "\n",
    "print(\"Best MAE\", study_1.best_value)\n",
    "print(\"Best params:\", study_1.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af98d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3807826a6ee63298feed0ad35eafa194",
     "grade": false,
     "grade_id": "cell-999478c9b4d31507",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.418099151103134\n",
    "Best params: {'learning_rate': 0.02967900426626908, 'subsample': 0.46364572682407335, 'colsample_bytree': 0.4107961679403244, 'min_child_samples': 20, 'num_leaves': 181}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a7dfd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aad0e00be261349e8b64ea11de40b77b",
     "grade": true,
     "grade_id": "cell-3392ef3c9a8216df",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the optimization results\n",
    "\n",
    "# The required hyperparameters should be optimized\n",
    "for param in [\"learning_rate\", \"subsample\", \"colsample_bytree\", \"min_child_samples\", \"num_leaves\"]:\n",
    "    assert param in study_1.best_trial.params, f\"Missing hyperparameter {param} from the objective function\"\n",
    "\n",
    "# The searching ranges should be correct\n",
    "assert study_1.best_trial.distributions.get(\"learning_rate\").log is True, \"learning_rate should be searched in a log scale\"\n",
    "for param in [\"learning_rate\", \"subsample\", \"colsample_bytree\"]:\n",
    "    assert isinstance(study_1.best_trial.distributions.get(param), optuna.distributions.FloatDistribution), f\"{param} should be a float number\"\n",
    "\n",
    "for param in [\"min_child_samples\", \"num_leaves\"]:\n",
    "    assert isinstance(study_1.best_trial.distributions.get(param), optuna.distributions.IntDistribution), f\"{param} should be an integer\"\n",
    "\n",
    "# The study should perform 100 trials\n",
    "assert len(study_1.trials) == 100, \"Wrong number of trials\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d95f03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4c55c2ea5bb1d1ae5894a5d4b034c2d",
     "grade": true,
     "grade_id": "cell-94d2125b6d76a2b7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the Optuna study results\n",
    "assert study_1.best_value < 0.42, \"Too large MAE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff141c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f54a9f42c6cdd8c40a02afe2f80cbf8c",
     "grade": false,
     "grade_id": "cell-a361e0c06af0a9fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Analyzing an Optuna study (2 points)\n",
    "Optuna offers utility functions for visualizing the optimization process (i.e., study history). For example, it can plot the hyperparameter importance and the relationship between a hyperparameter and the objective. In this assignment, you need to analyze the study created in Assignment 1, adjust the search ranges of some hyperparameters to obtain better MAE. Detailed instructions will be provided later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc3bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b20258ee0a86bd6d27553b9d85fb2be",
     "grade": false,
     "grade_id": "cell-6143e15920dad03d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# Configure Jupyter Notebook to render plotly figures drawn by Optuna\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7db4ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccc1b74b10e62c86d1bda2031f145c21",
     "grade": false,
     "grade_id": "cell-00c82c5f09579eb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2a) Hyperparameter importance\n",
    "Implement a function `show_param_importances` that loads an Optuna study from a storage and return a plot showing importance of the hyperparameters in the study. Similar to the tutorial, use the [FanovaImportanceEvaluator](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.FanovaImportanceEvaluator.html#optuna.importance.FanovaImportanceEvaluator) as the importance evaluator and set `RANDOM_STATE` as the seed for the evaluator for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de9c8f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2caa9c80bd17e2cb75b6fd7ab1dc1330",
     "grade": false,
     "grade_id": "cell-56495b0b9ae677e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the hyperparameter importance\n",
    "def show_param_importances(study_name: str, storage: str) -> plotly.graph_objs.Figure:\n",
    "    \"\"\"\n",
    "    Plot the hyperparameter importance of a study.\n",
    "    Args:\n",
    "        study_name: The name of the study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "    Returns:\n",
    "        A plotly Figure object.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "param_importance_fig_wine = show_param_importances(study_name=study_name_1, storage=storage)\n",
    "param_importance_fig_wine.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f056c23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7287095cde0bbee523b24599ad476da0",
     "grade": false,
     "grade_id": "cell-d181f40585b27273",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, put the names of the three most important hyperparameters in to a list named `important_hyperparams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cad0a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57bab6be38c1392a94fa47119a5c4db4",
     "grade": false,
     "grade_id": "cell-1fec64bc9fcdc304",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: important_hyperparams = ...\n",
    "### START CODE HERE\n",
    "raise NotImplementedError()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ef1b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f180c87336301806e432157c4c14ddb3",
     "grade": true,
     "grade_id": "cell-0ac20cc5651d27ca",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(param_importance_fig_wine, plotly.graph_objs.Figure), \"Incorrect importance plot\"\n",
    "assert len(important_hyperparams) == 3, \"Incorrect number of important hyperparameters\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a8161",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f2fc21a2710944e798c2e8a65e66726",
     "grade": false,
     "grade_id": "cell-78714ff483c9bad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2b) The impact of hyperparameters\n",
    "Complete the `plot_relationship_between_hyperparams_and_obj` function that loads an Optuna study from a storage and return a plot showing the relationships of the most three important hyperparameters and the objective in a slice plot. \n",
    "\n",
    "Hint: [How to plot the relationship between a hyperparameter and the objective?](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_slice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dd0bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "615724b6a8d3a40ea54f8ad55bbbad11",
     "grade": false,
     "grade_id": "cell-cd0ab0fecac2d570",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def show_relationship_between_hyperparams_and_obj(study_name: str, storage: str, important_hyperparams: List[str]) -> plotly.graph_objs.Figure:\n",
    "    \"\"\"\n",
    "    Plot the relationship between the hyperparameters and the objective function.\n",
    "    Args:\n",
    "        study_name: The name of the study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        important_hyperparams: A list of hyperparameters that are considered important.\n",
    "    Returns:\n",
    "        A plotly Figure object.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "slice_fig_wine = show_relationship_between_hyperparams_and_obj(study_name=study_name_1, storage=storage, important_hyperparams=important_hyperparams)\n",
    "slice_fig_wine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58d9a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7cd8e85416d3c701bd43b26b62a0156",
     "grade": true,
     "grade_id": "cell-47c46025a5acbdea",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(slice_fig_wine, plotly.graph_objs.Figure), \"Incorrect slice plot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84985c-4c3c-4ade-b9b8-2f9366ca7955",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "108a203e92ad19ed3a79030341f9e083",
     "grade": false,
     "grade_id": "cell-6eea914c144ea5bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question for Assignment 2b\n",
    "Now you know the most three important hyperparameters that affect the objective value. Looking at the positions of the points in the slice plot, which area the points that resulted in better MAE are concentrated on? How would you adjust the search ranges of these three hyperparameters in the next study? Please put your answer in your PDF file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b6a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7be6c85098b5a7bc31d5b4009c4cd746",
     "grade": false,
     "grade_id": "cell-13a5f28b8234856a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2c) An improved Optuna study\n",
    "In this assignment, you have two tasks. First, complete another objective function `objective_func_2`, where you need to adjust the search ranges of the most three important hyperparameters to improve MAE (keep values/search ranges of other hyperparameters as the same as in Assignment 1).\n",
    "\n",
    "Then complete the `run_improved_study` that starts and runs another Optuna study. Detailed requirements are listed below:\n",
    "- Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use `RANDOM_SEED` as the seed of the sampler. \n",
    "- The Optuna study should have a name specified by the `new_study_name` argument, use the objective function given as the `objective_func` argument and perform `n_trials` trials.\n",
    "- The study history should be persisted in a relational database specified by the `storage` argument so that the study can be loaded and analyzed later. \n",
    "- You should start the study using the best hyperparameter combination of a previous study. The previous study should be loaded from the given storage using the study name given as the `prev_study_name` argument.\n",
    "- The trials should be tracked in an MLflow experiment. \n",
    "- The function should finally return the study.\n",
    "\n",
    "**Hints**: \n",
    "- Now you know the most three important hyperparameters that affect the objective value. Looking at the positions of the points in the slice plot, which areas the points that resulted in better MAE are concentrated on? \n",
    "- [How to log Optuna trials to MLflow?](https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html)\n",
    "- To start the new study using the best hyperparameter combination of the previous study, you can 1) load the previous study whose name is given as the `prev_study_name` argument, 2) retrieve the optimal hyperparameters combination of the previous study, 3) create a new study, 4) insert a trial with the best hyperparameter values of the previous study into the new study, and then start the optimization of the new study.\n",
    "\n",
    "\n",
    "You may also find the following links helpful:\n",
    "- [optuna.load_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.load_study.html)\n",
    "- [study.best_params](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.best_params)\n",
    "- [enqueue_trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.enqueue_trial). \n",
    "\n",
    "**Note**: If you want to delete all the trials in MLflow, don't delete the experiment, as deleting the experiment using the UI will not really delete the experiment in the PostgreSQL database used by MLflow, which will cause problems when recording trials under an experiment with the same name as the deleted experiment. Instead, you can keep the experiment and delete the trials, as shown in the image below. (If you feel like permanently deleting MLflow experiments from the PostgreSQL database, please check the `delete_from_mlflow.md` file in Week1 tutorials.\n",
    "\n",
    "![](./images/mlflow-delete-trials.jpg)\n",
    "\n",
    "<details>\n",
    "    <summary>If callback and decorator are new to you...</summary>\n",
    "    <p>Briefly speaking, both callbacks and decorators are used to modify or enhance the behavior of another function without modifying its original source code. A callback is a function that is typically provided as an argument to another function. A decorator typically takes another function as an argument. Feel free to google more about them. </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee89bc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c367b4cdebe2f19fc1361949d0dc020",
     "grade": false,
     "grade_id": "cell-a0685c1f324066ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def improved_objective_func(trial):\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "def run_improved_study(prev_study_name: str, new_study_name: str, storage: str, objective_func: callable, n_trials: int) -> optuna.study.Study:\n",
    "    \"\"\"\n",
    "    Create and run an Optuna study based on a previous study.\n",
    "    Args:\n",
    "        prev_study_name: The name of the previous study.\n",
    "        new_study_name: The name of the new study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        objective_func: The objective function to be optimized.\n",
    "        n_trials: The number of trials the study should perform.\n",
    "    Returns:   \n",
    "        An Optuna Study object.\n",
    "    \"\"\"\n",
    "    # Some cleanup before starting the new study\n",
    "    # Delete the study if it already exists\n",
    "    if new_study_name in optuna.get_all_study_names(storage=storage):\n",
    "        optuna.delete_study(study_name=new_study_name, storage=storage)\n",
    "\n",
    "    mlflow_exp = mlflow.get_experiment_by_name(new_study_name)\n",
    "    if mlflow_exp is not None:\n",
    "        # Delete all old trials in the experiment\n",
    "        for run in mlflow.search_runs(mlflow_exp.experiment_id, output_format=\"list\"):\n",
    "            mlflow.delete_run(run.info.run_id)\n",
    "        \n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad27b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76e4c2be557ca64c561b5554d17c5ebf",
     "grade": false,
     "grade_id": "cell-d5ab13033714df18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Name of the new study\n",
    "new_study_name = \"lgbm-wine-2\"\n",
    "\n",
    "study_2 = (\n",
    "    run_improved_study(\n",
    "        prev_study_name=study_name_1,\n",
    "        new_study_name=new_study_name,\n",
    "        storage=storage,\n",
    "        objective_func=improved_objective_func,\n",
    "        n_trials=20,\n",
    "    )\n",
    "    if not is_being_graded()\n",
    "    else optuna.load_study(study_name=new_study_name, storage=storage)\n",
    ")\n",
    "print(\"Best MAE\", study_2.best_value)\n",
    "print(\"Best params:\", study_2.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7014c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba50d3ccb2c5ebc45d94186f81e6bdfa",
     "grade": false,
     "grade_id": "cell-f4730c7f565c1394",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.386448234091213\n",
    "Best params: {'learning_rate': 0.020120960308154467, 'subsample': 0.9215993238868946, 'colsample_bytree': 0.825439643398257, 'min_child_samples': 10, 'num_leaves': 855}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0d1d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc8c03f6b53737bff7db85b98164399",
     "grade": true,
     "grade_id": "cell-37fd5bed7f79740e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# New MAE should be smaller than the previous one\n",
    "assert study_2.best_value < study_1.best_value, \"The new MAE should be smaller than the previous one\"\n",
    "\n",
    "# The new study should have 20 trials\n",
    "assert len(study_2.trials) == 20, \"Wrong number of trials\"\n",
    "\n",
    "# The first trial should be the same as the best trial of the previous study\n",
    "assert study_2.trials[0].values[0] == study_1.best_value\n",
    "\n",
    "# The ranges of at least one of the three most important hyperparameters should be changed\n",
    "param_changed = False\n",
    "for param in important_hyperparams:\n",
    "    new_distribution = study_2.best_trial.distributions.get(param)\n",
    "    old_distribution = study_1.best_trial.distributions.get(param)\n",
    "    if new_distribution.low != old_distribution.low or new_distribution.high != old_distribution.high:\n",
    "        param_changed = True\n",
    "        break\n",
    "assert param_changed, \"The ranges of at least one of the three most important hyperparameters should be changed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249bbab-e23e-4c8c-a411-82f80b5630cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bb2724a1e489a74aea93c570c6234ca",
     "grade": false,
     "grade_id": "cell-e817787ef6626f78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Screenshot for Assignment 2c\n",
    "Capture a screenshot of the trails of the \"lgbm-wine-2\" study in MLflow UI. Please include the metrics and parameters of each trial in your screenshot.\n",
    "\n",
    "<details>\n",
    "    <summary>Example:</summary>\n",
    "    <img src=\"./images/trials-mlflow.png\" width=1000/>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a66a70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0890502f27353e504ec7bb2e8131257",
     "grade": false,
     "grade_id": "cell-3b864625da24f09c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: More about MLflow (2 points)\n",
    "### 3a) Find the MLflow run with the best hyperparameter combination\n",
    "In assignment 2c), you found the best hyperparameter combination for your model. Now, your task is to complete the `find_best_run_id` function. The function receives an MLflow Experiment name as the argument and returns the best MLflow Run ID that resulted in the best hyperparameter combination. In other words, this function should find the MLflow Run with the smallest MAE. \n",
    "\n",
    "You may find the following MLflow docs useful:\n",
    "- [How to retrieve an MLflow Experiment given an Experiment name?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=get_experiment_by_name#mlflow.get_experiment_by_name)\n",
    "- [How to search MLflow Runs inside an MLflow Experiment?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=search_runs#mlflow.search_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98b8e2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f770718234ce98bb7d9e903b9fc45d0a",
     "grade": false,
     "grade_id": "cell-49f6069d6e076b5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_best_run_id(mlflow_experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the ID of the MLflow run with the smallest MAE\n",
    "    Args:\n",
    "        mlflow_experiment_name: The name of the MLflow experiment where the run should be found\n",
    "    Return:\n",
    "        An MLflow run ID\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b92b09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78cb191d3641479f7afbc9241829a38d",
     "grade": true,
     "grade_id": "cell-5aa7e5ba272a4100",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# From your MLflow UI you can check whether the printed MLflow Run ID really produced the smallest MAE\n",
    "if not is_being_graded():\n",
    "    best_run_id = find_best_run_id(mlflow_experiment_name=new_study_name)\n",
    "    print(f\"The best MLflow Run ID is: {best_run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee8d8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7defc2d30672332f15cbac88c3eecae9",
     "grade": false,
     "grade_id": "cell-ac5afe4ff1ac28e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3b) Train a model using the best hyperparameter combination\n",
    "Your task is to train a model using the best hyperparameter combination found in Assignment 2c). You also need to upload and register the model to MLflow, the model needs to be associated with the MLflow Run where the optimal hyperparameter combination was found.\n",
    "\n",
    "For example, suppose running the \"lgbm-wine-2\" study of Assignment 2 created an MLflow Run 14 where the best hyperparameter combination was found, the model created in this assignment needs to be associated with MLflow Run 14, as shown below:\n",
    "\n",
    "<img src=\"./images/mlflow-best-run.png\" width=1200/>\n",
    "\n",
    "<img src=\"./images/mlflow-best-model.png\" width=1200/>\n",
    "\n",
    "Hints:\n",
    "- You may find the following function helpful: [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter).\n",
    "- It would probably be more convenient to retrieve the best hyperparameter combination using `optuna.load_study` rather than from the Mlflow Run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700a4a5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7281b465add4ff602f88c1888ee1016",
     "grade": false,
     "grade_id": "cell-5167cdef32da16d4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_optimized_model(study_name: str, storage: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Train a model with the best hyperparameters found by Optuna.\n",
    "    Args:\n",
    "        study_name: The name of the Optuna study where the best hyperparameters have been found. This is also the name of the MLflow experiment.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        model_name: The name of the registered MLflow model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4b942",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffaf93d09aa2681fb984217c4c598a6a",
     "grade": true,
     "grade_id": "cell-42706fe4f4834bc6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# From your MLflow UI you can make sure that the registered model is associated with the \n",
    "if not is_being_graded():\n",
    "    train_optimized_model(study_name=new_study_name, storage=storage, model_name=\"optuna-lgbm-wine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4611d2-fa0e-4d9f-8a58-dea5260deb6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4fb7c94825d95ac820b6c6de65c61dc",
     "grade": false,
     "grade_id": "cell-b170d027be4a30de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Screenshots for Assignment 3b)\n",
    "Submit the screenshots of the registered model version info and the corresponding MLflow run. You can take the images in the assignment instructions above as an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dfc29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bab90346c90b6e12cabf680887036ae",
     "grade": false,
     "grade_id": "cell-1a0420c24806c592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Conditional search space with Optuna (2 points)\n",
    "Complete the objective function `objective_func_multimodel`. The target is to optimize the hyperparameters of a LightGBM and an [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) regression model for the red wine quality prediction use case and see which model type is better in the use case. Similar to the previous assignments, the objective is also a smaller MAE. The hyperparameters to be tuned and their search ranges are given below. Use TPESampler with `RANDOM_SEED` as the random seed.\n",
    "\n",
    "**LightGBM**: \n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 1.0]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_samples | A leaf node should have at least `min_child_samples` data points to be further splitted. | integer | [1, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value) \n",
    "\n",
    "**XGBoost**\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | Same as LightGBM.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | Same as LightGBM.             | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | Same as LightGBM.            | float   | [0.05, 1.0]                                                              |                                         \n",
    "| colsample_bytree  | Same as LightGBM.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_weight | Minimum sum of instance weight needed for a leaf mode to be further splitted (similar to min_child_samples in LightGBN). | integer | [1, 100]                                                                |\n",
    "| max_depth        | Max depth of a single tree                                       | integer | [1, 10]                                                                |\n",
    "| random_state      | Same as LightGBM.                                   | integer | RANDOM_SEED (fixed value)    \n",
    "\n",
    "**Notes**: \n",
    "- When specifying the search ranges for the model type, please use **\"model_type\"** as the parameter name that indicates the model type and `[\"lgbm\", \"xgb\"]` as the value candidates.\n",
    "- Some of the hyperparameters to be optimized used by the LightGBM and XGBoost models share the same names. In the assignment, these hyperparameters are `learning_rate, subsample, colsample_bytree`. You need to give these hyperparameters a unique name when defining their search ranges using `trial.suggest_*` so that Optuna can properly optimize the correct hyperparameters for each different model. Please rename the hyperparameters as `<hyperparameter-name>_lgbm` for the LightGBM model and `<hyperparameter-name>_xgb` for the XGBoost model, such as `learning_rate_lgbm` and `learning_rate_xgb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9bf03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef66c88d48c448b4804abb08c9a06a91",
     "grade": false,
     "grade_id": "cell-c778c2cd2bd9ad2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def objective_func_multimodel(trial):\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65a160",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83bfc316012ed588749cd53891ffa95e",
     "grade": true,
     "grade_id": "cell-dd641c2f99f55316",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "multimodel_study_name = \"multimodel-wine\"\n",
    "multimodel_study = run_study(\n",
    "    study_name=multimodel_study_name,\n",
    "    storage=None,\n",
    "    objective_func=objective_func_multimodel,\n",
    "    n_trials=10,\n",
    ")\n",
    "\n",
    "print(\"Best MAE\", multimodel_study.best_value)\n",
    "print(\"Best params:\", multimodel_study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded721db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57e686afb3690d53a9044624a06b0640",
     "grade": false,
     "grade_id": "cell-8bb4c463904c3328",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.42899067751612513\n",
    "Best params: {'model_type': 'lgbm', 'learning_rate_lgbm': 0.012172847081122434, 'subsample_lgbm': 0.18387801372602453, 'colsample_bytree_lgbm': 0.8120871317163377, 'min_child_samples': 8, 'num_leaves': 1011}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eabfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec95bd9ad5e47f0f2e0483332eb63af1",
     "grade": false,
     "grade_id": "cell-02c2bb77613a18bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 5: Ensemble averaging (2 points)\n",
    "Complete the objective function `objective_func_ensemble`. The purpose is to Optuna to find the optimal weight combination to combine the predictions of an Sklearn's RandomForest, a XGBoost, and a LightGBM model to obtain better predictions for the red wine quality us case. Detailed instructions are given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdb0a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0bd13b7b97dda633005e175675c076e",
     "grade": false,
     "grade_id": "cell-2c406b700d88e7d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before going to the assignment, first train three models for the red wine quality use case using Sklearn's RandomForest, XGBoost, and LightGBM and evaluate MAE of each model. For simplicity, the default configurations of the hyperparameters are used except for \"random_state\" which is set as `RANDOM_SEED` as in the previous assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca98e6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736c9b6f5fc140438fa6051d605dcfba",
     "grade": false,
     "grade_id": "cell-6b13961c39f1ce32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "xgb_model = xgb.XGBRegressor(random_state=RANDOM_SEED)\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=RANDOM_SEED, verbose=-1)\n",
    "\n",
    "model_names = [\"rf_model\", \"xgb_model\", \"lgbm_model\"]\n",
    "\n",
    "for name in model_names:\n",
    "    model = eval(name)\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    print(f\"{name}: {mean_absolute_error(test_y, predictions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e656f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17180485444f4b234e890e48e4f1074f",
     "grade": false,
     "grade_id": "cell-af7b23190fa28641",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see the best performing model is the random forest one, with an MAE of 0.4228.\n",
    "\n",
    "Now, it is time to combine the predictions of these models to improve the final prediction. In this assignment, your task is to complete the objective function `objective_func_ensemble`. The target is to find the best weight combination for the three models that have been trained in the cell above to obtain smaller MAE. Use 3 as the [step of discretization](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) when searching weight for each model. The search range of the weight should be [1, 100]. Use TPESampler (with `RANDOM_SEED` as the seed) in the study.\n",
    "\n",
    "**Note**: Please use \"rf_model\", \"xgb_model\", and \"lgbm_model\" as the names of the parameters that specify the weights of the random forest, XGBoost, and LightGBM models, respectively. E.g., \n",
    "```python\n",
    "study = run_study(study_name=\"ensemble-wine\", storage=None, objective_func=objective_func_ensemble, n_trials=10)\n",
    "print(study.best_trial.params)\n",
    "# Example_output:\n",
    "# {'rf_model': 46, 'xgb_model': 79, 'lgbm_model': 19}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab33a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b52e1953f25572083c6e29a99a36e2b",
     "grade": false,
     "grade_id": "cell-c58cfc6b2eb0a37f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "all_predictions = {name: (eval(name)).predict(test_x) for name in model_names}\n",
    "\n",
    "# Define the objective function\n",
    "def objective_func_ensemble(trial):\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65360ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9aac4b653c3c2aee81a58e077d81e92f",
     "grade": false,
     "grade_id": "cell-49f6332e4ca5b339",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "study_ensemble = run_study(study_name=\"ensemble-wine\", storage=None, objective_func=objective_func_ensemble, n_trials=10)\n",
    "\n",
    "print(\"Best MAE\", study_ensemble.best_value)\n",
    "print(\"Best params:\", study_ensemble.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad76c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f5e44de11c35f8e0fd5fc75a9dbcb92",
     "grade": false,
     "grade_id": "cell-28abe759ddfbdf3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.41523636202081865\n",
    "Best params: {'rf_model': 46, 'xgb_model': 79, 'lgbm_model': 19}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0ad36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d308dd8b5232627a44e454d5c759ab9",
     "grade": true,
     "grade_id": "cell-f6df020e710b6302",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    study_ensemble.best_value < 0.4228\n",
    "), \"The MAE should be smaller than 0.4228 (the MAE of the best single model)\"\n",
    "\n",
    "assert set(study_ensemble.best_trial.params.keys()) == set([\"rf_model\", \"xgb_model\", \"lgbm_model\"]), \"Incorret parameter names\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70090913",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb8641167064af2d5fe631b4ca9f194d",
     "grade": false,
     "grade_id": "cell-2489dd0853cd1559",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 6: Scale up Optuna using Ray Tune\n",
    "In this assignment, you need to parallelize the Optuna hyperparameter optimization process using Ray Tune. Specifically, you will do this in three steps:\n",
    "1. Complete the `trainable` function to define a Trainable. You need to specify the model, the model training process, and the metric to be optimized. Similar to Assignment 1, the model is an LightGBM regressor, and the metric to be optimized is MAE. \n",
    "1. Complete the `create_search_algo` function to define search spaces and the search algorithm. Use [OptunaSearch](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.optuna.OptunaSearch.html) here. The search spaces should be the same as Assignment 1. Use the TPESampler as you did in Assignment 1.\n",
    "1. Complete the `create_ray_tuner` function to define a Tuner that launches hyperparameter optimization trials. This function receives a Boolean argument named `parallel`. If `parallel` is set to False, the Tuner should perform the hyperparameter optimization trials one by one. Otherwise, the Tuner should use all available CPUs on your machine to run the trials concurrently. You'll find the details of the arguments passed to the function in the function docs. \n",
    "\n",
    "**Notes**:\n",
    "- Remember to specify the random seed (`RANDOM_SEED`) for your model and sampler.\n",
    "- When define the Trainable, please use **\"mae\"** as the metric name when report the metric to the Tuner. \n",
    "- The actual number of the concurrent jobs doesn't matter (as long as it's larger than 1) as it depends on the number of CPUs available on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c5dd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "096aa3f4d3d268b85fd8bec072b03423",
     "grade": false,
     "grade_id": "cell-49483c1930dddf4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def trainable(config: Dict):\n",
    "    \"\"\"\n",
    "    Defines a model using the given configuration, trains it, and report the metric.\n",
    "    Args:\n",
    "        config: A dictionary containing the hyperparameters to be tuned\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def create_search_algo() -> OptunaSearch:\n",
    "    \"\"\"\n",
    "    Create an Optuna search algorithm.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def create_ray_tuner(\n",
    "    trainable: callable, algo: OptunaSearch, n_trials: int, parallel: bool = False\n",
    ") -> tune.Tuner:\n",
    "    \"\"\"\n",
    "    Create a Ray Tune Tuner\n",
    "    Args:\n",
    "        trainable: The Trainable that specifies the objective.\n",
    "        algo: The search algorithm.\n",
    "        n_trials: The number of trials.\n",
    "        parallel: Whether to run the trials in parallel.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f49de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80fe383040be89e63ca18982866d63db",
     "grade": true,
     "grade_id": "cell-97d1c69ad8af53c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# When the Tuner is running trials sequentially, the best hyperparameters should be the same as the ones found by the Optuna study in the first assignment\n",
    "tuner = create_ray_tuner(trainable=trainable, algo=create_search_algo(), n_trials=10, parallel=False)\n",
    "results = tuner.fit()\n",
    "original_optuna_study = run_study(study_name=\"original\", storage=None, objective_func=objective_func, n_trials=10)\n",
    "assert results.get_best_result(metric=\"mae\", mode=\"min\").metrics.get(\"mae\") == original_optuna_study.best_value, \"Incorrect best hyperparameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32426049",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b199f3ae2274ea7df3e95247d4118bd6",
     "grade": true,
     "grade_id": "cell-7269c75a99605c04",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Mock a slow training process\n",
    "def slow_trainable(config):\n",
    "    time.sleep(10)\n",
    "    return ({\"mae\": random.random()})\n",
    "\n",
    "tuner_parallelized = create_ray_tuner(trainable=slow_trainable, algo=create_search_algo(), n_trials=5, parallel=True)\n",
    "start_time = time.time()\n",
    "results = tuner_parallelized.fit()\n",
    "end_time = time.time()\n",
    "\n",
    "assert end_time - start_time < 50, \"It should take less than 50s to complete the optimization in the parallel mode\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50105b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "068a598f580995f00c138679043df4f4",
     "grade": false,
     "grade_id": "cell-10b4b9d0c0640ad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Wrap-up\n",
    "Please include the following files in your submission:\n",
    "- This Jupyter notebook (`week3_assignments.ipynb`)\n",
    "- The \"optuna.sqlite3\" database file\n",
    "- The PDF file containing your answers for Assignment 2b and screenshots for Assignments 2c and 3b\n",
    "\n",
    "**N.B.** Before making your submission, please check that your notebook and database files are named **exactly** as specified here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
